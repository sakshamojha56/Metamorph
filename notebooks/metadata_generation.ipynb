{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae903734",
   "metadata": {},
   "source": [
    "# Metamorph: Automated Metadata Generation System\n",
    "\n",
    "This notebook demonstrates the complete process of building an automated metadata generation system from scratch. The system processes various document formats, extracts meaningful content, and generates structured metadata that can be used for better document management and insights.\n",
    "\n",
    "## Objectives\n",
    "- Extract text content from various document formats (PDF, DOCX, TXT)\n",
    "- Implement OCR for images and scanned documents\n",
    "- Identify semantic content and key sections\n",
    "- Generate structured metadata (title, keywords, entities, etc.)\n",
    "- Build a simple web interface for document upload and metadata display\n",
    "- Package the system for deployment\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Document processing libraries\n",
    "import PyPDF2\n",
    "import docx\n",
    "from PIL import Image\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For OCR\n",
    "import pytesseract\n",
    "\n",
    "# For visualization in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For web interface (Streamlit)\n",
    "# Note: This will run in a separate file, not in this notebook\n",
    "# import streamlit as st\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model loaded successfully\")\n",
    "except:\n",
    "    print(\"spaCy model not found. Downloading model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model loaded successfully\")\n",
    "\n",
    "# Create directory for sample documents and output\n",
    "os.makedirs('sample_docs', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee9911",
   "metadata": {},
   "source": [
    "## Document Content Extraction\n",
    "\n",
    "In this section, we'll build functions to extract text content from various document formats:\n",
    "1. PDF files using PyPDF2\n",
    "2. DOCX files using python-docx\n",
    "3. TXT files using standard Python file operations\n",
    "\n",
    "Let's create a document processor class that can handle different file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78224c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Class for processing various document types and extracting text\"\"\"\n",
    "    \n",
    "    def extract_text(self, file_path):\n",
    "        \"\"\"\n",
    "        Extract text from document based on file extension\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the document\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted text from the document\n",
    "        \"\"\"\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "        \n",
    "        if file_extension == 'pdf':\n",
    "            return self._extract_from_pdf(file_path)\n",
    "        elif file_extension == 'docx':\n",
    "            return self._extract_from_docx(file_path)\n",
    "        elif file_extension == 'txt':\n",
    "            return self._extract_from_txt(file_path)\n",
    "        elif file_extension in ['png', 'jpg', 'jpeg']:\n",
    "            return self._extract_from_image(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_extension}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _extract_from_pdf(self, file_path):\n",
    "        \"\"\"Extract text from PDF files\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num]\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "            if text.strip() == \"\":\n",
    "                print(\"No text extracted from PDF. It might be a scanned document requiring OCR.\")\n",
    "                return self._perform_ocr_on_pdf(file_path)\n",
    "                \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _perform_ocr_on_pdf(self, file_path):\n",
    "        \"\"\"Perform OCR on PDF if regular extraction fails (likely a scanned document)\"\"\"\n",
    "        try:\n",
    "            # This is a simplified approach - in production, you'd use a library like pdf2image\n",
    "            # to convert all pages to images and then process them\n",
    "            print(\"Attempting OCR on PDF...\")\n",
    "            images = self._pdf_to_images(file_path)\n",
    "            text = \"\"\n",
    "            for img in images:\n",
    "                text += pytesseract.image_to_string(img) + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error performing OCR on PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _pdf_to_images(self, file_path):\n",
    "        \"\"\"Convert PDF to list of images - simplified implementation\"\"\"\n",
    "        # In production, you would use pdf2image or a similar library\n",
    "        # For this notebook, we'll return an empty list to avoid extra dependencies\n",
    "        print(\"PDF to image conversion requires additional libraries like pdf2image.\")\n",
    "        print(\"In a production environment, install: pip install pdf2image\")\n",
    "        return []\n",
    "    \n",
    "    def _extract_from_docx(self, file_path):\n",
    "        \"\"\"Extract text from DOCX files\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            doc = docx.Document(file_path)\n",
    "            for para in doc.paragraphs:\n",
    "                text += para.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from DOCX: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _extract_from_txt(self, file_path):\n",
    "        \"\"\"Extract text from TXT files\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                    return file.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting text from TXT: {e}\")\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from TXT: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _extract_from_image(self, file_path):\n",
    "        \"\"\"Extract text from image files using OCR\"\"\"\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            text = pytesseract.image_to_string(img)\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from image: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and preprocess the extracted text\n",
    "        \n",
    "        Args:\n",
    "            text (str): Extracted text from document\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "            \n",
    "        # Remove excess whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters and numbers (optional - depends on use case)\n",
    "        # text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "# Create an instance of the document processor\n",
    "doc_processor = DocumentProcessor()\n",
    "\n",
    "# Let's create a simple sample text file to test our processor\n",
    "sample_text = \"\"\"\n",
    "Metamorph: Automated Metadata Generation System\n",
    "\n",
    "This is a sample text document to test the document processor.\n",
    "It contains multiple paragraphs and some formatting.\n",
    "\n",
    "Key features of our system:\n",
    "- Multi-format support\n",
    "- Semantic analysis\n",
    "- Metadata generation\n",
    "- Web interface\n",
    "\n",
    "Â© 2025 Metamorph\n",
    "\"\"\"\n",
    "\n",
    "with open('sample_docs/sample.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Test the document processor with our sample text file\n",
    "extracted_text = doc_processor.extract_text('sample_docs/sample.txt')\n",
    "print(\"Extracted text from sample.txt:\")\n",
    "print(\"-\" * 50)\n",
    "print(extracted_text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Clean the extracted text\n",
    "cleaned_text = doc_processor.clean_text(extracted_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(\"-\" * 50)\n",
    "print(cleaned_text)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ae583",
   "metadata": {},
   "source": [
    "## Optical Character Recognition (OCR)\n",
    "\n",
    "For documents that contain images or are scanned PDFs, we need to use OCR (Optical Character Recognition) to extract the text. We've already included OCR capabilities in our DocumentProcessor class, but let's explore OCR in more detail and demonstrate its capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615389d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate OCR capabilities by:\n",
    "    1. Creating a simple image with text\n",
    "    2. Saving it to a file\n",
    "    3. Applying OCR to extract the text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if pytesseract is properly installed and configured\n",
    "        pytesseract.get_tesseract_version()\n",
    "        print(f\"Tesseract version: {pytesseract.get_tesseract_version()}\")\n",
    "        \n",
    "        # Create a simple image with text using PIL\n",
    "        img = Image.new('RGB', (800, 200), color=(255, 255, 255))\n",
    "        from PIL import ImageDraw, ImageFont\n",
    "        \n",
    "        # Try to use a default font, or fall back to default\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"Arial\", 24)\n",
    "        except:\n",
    "            # Use default font if Arial is not available\n",
    "            font = ImageFont.load_default()\n",
    "            \n",
    "        draw = ImageDraw.Draw(img)\n",
    "        text = \"This is a test image for OCR. Metamorph can extract text from images.\"\n",
    "        draw.text((50, 80), text, fill=(0, 0, 0), font=font)\n",
    "        \n",
    "        # Save the image\n",
    "        img_path = 'sample_docs/ocr_test.png'\n",
    "        img.save(img_path)\n",
    "        print(f\"Created test image at {img_path}\")\n",
    "        \n",
    "        # Apply OCR\n",
    "        extracted_text = pytesseract.image_to_string(img)\n",
    "        print(\"\\nExtracted text from image:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(extracted_text)\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Display the image\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title('Test Image for OCR')\n",
    "        plt.show()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"OCR demo failed: {e}\")\n",
    "        print(\"\\nNote: For OCR to work properly, you need to have Tesseract installed on your system.\")\n",
    "        print(\"Installation instructions:\")\n",
    "        print(\"- Ubuntu/Debian: sudo apt-get install tesseract-ocr\")\n",
    "        print(\"- macOS: brew install tesseract\")\n",
    "        print(\"- Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Run the OCR demo\n",
    "ocr_success = ocr_demo()\n",
    "\n",
    "# If OCR demo fails, create a function to simulate OCR for the remainder of the notebook\n",
    "if not ocr_success:\n",
    "    def simulate_ocr(image_path_or_object):\n",
    "        \"\"\"Simulate OCR for demonstration purposes\"\"\"\n",
    "        print(\"Using simulated OCR since actual OCR is not available\")\n",
    "        return \"This is simulated OCR text. In a real environment, actual text would be extracted from the image.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f49d9e",
   "metadata": {},
   "source": [
    "## Semantic Content Identification\n",
    "\n",
    "Now that we can extract text from documents, we need to identify the semantically meaningful parts of the content. This includes:\n",
    "\n",
    "1. Identifying important entities (people, organizations, locations, dates, etc.)\n",
    "2. Extracting key sentences and phrases\n",
    "3. Identifying document structure (title, sections, etc.)\n",
    "\n",
    "We'll use spaCy for named entity recognition and other NLP techniques to extract meaningful information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf237ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticAnalyzer:\n",
    "    \"\"\"Class for analyzing document content and extracting semantic information\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the analyzer with NLP models and resources\"\"\"\n",
    "        self.nlp = nlp  # Using the already loaded spaCy model\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"\n",
    "        Extract named entities from text using spaCy\n",
    "        \n",
    "        Args:\n",
    "            text (str): Document text\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of entity types and their values\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {}\n",
    "            \n",
    "        # Process text with spaCy\n",
    "        doc = self.nlp(text[:100000])  # Limit text length to avoid memory issues\n",
    "        \n",
    "        # Group entities by type\n",
    "        entities = {\n",
    "            \"PERSON\": [],\n",
    "            \"ORG\": [],\n",
    "            \"GPE\": [],  # Countries, cities, states\n",
    "            \"DATE\": [],\n",
    "            \"MISC\": []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"PERSON\"]:\n",
    "                entities[\"PERSON\"].append(ent.text)\n",
    "            elif ent.label_ in [\"ORG\"]:\n",
    "                entities[\"ORG\"].append(ent.text)\n",
    "            elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "                entities[\"GPE\"].append(ent.text)\n",
    "            elif ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "                entities[\"DATE\"].append(ent.text)\n",
    "            else:\n",
    "                entities[\"MISC\"].append(ent.text)\n",
    "        \n",
    "        # Remove duplicates and limit length\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))[:10]\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def extract_keywords(self, text, max_keywords=10):\n",
    "        \"\"\"\n",
    "        Extract keywords using TF-IDF\n",
    "        \n",
    "        Args:\n",
    "            text (str): Document text\n",
    "            max_keywords (int): Maximum number of keywords to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of keywords with scores\n",
    "        \"\"\"\n",
    "        if not text or len(text.split()) < 20:\n",
    "            return self._extract_frequent_words(text, max_keywords)\n",
    "        \n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                max_df=0.85,\n",
    "                min_df=2,\n",
    "                stop_words='english',\n",
    "                use_idf=True,\n",
    "                ngram_range=(1, 2)\n",
    "            )\n",
    "            \n",
    "            # Create artificial documents by splitting text into chunks\n",
    "            sentences = sent_tokenize(text)\n",
    "            documents = [text]  # Original full text\n",
    "            \n",
    "            # Group sentences into chunks\n",
    "            chunk_size = max(5, len(sentences) // 5)\n",
    "            for i in range(0, len(sentences), chunk_size):\n",
    "                chunk = \" \".join(sentences[i:i+chunk_size])\n",
    "                if chunk:\n",
    "                    documents.append(chunk)\n",
    "            \n",
    "            tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Get scores from the first document (the full text)\n",
    "            tfidf_scores = zip(feature_names, tfidf_matrix[0].toarray()[0])\n",
    "            sorted_keywords = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Filter keywords to ensure they are meaningful\n",
    "            keywords = []\n",
    "            for keyword, score in sorted_keywords:\n",
    "                if score > 0.01 and len(keyword) > 2:\n",
    "                    keywords.append({\n",
    "                        \"text\": keyword,\n",
    "                        \"score\": float(score)\n",
    "                    })\n",
    "                if len(keywords) >= max_keywords:\n",
    "                    break\n",
    "            \n",
    "            return keywords\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting keywords: {e}\")\n",
    "            return self._extract_frequent_words(text, max_keywords)\n",
    "    \n",
    "    def _extract_frequent_words(self, text, max_words=10):\n",
    "        \"\"\"\n",
    "        Extract most frequent words as a fallback method\n",
    "        \n",
    "        Args:\n",
    "            text (str): Document text\n",
    "            max_words (int): Maximum number of words to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of words with scores\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            # Tokenize and clean\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            tokens = [word for word in tokens if word.isalpha() and word not in self.stop_words and len(word) > 2]\n",
    "            \n",
    "            # Count frequency\n",
    "            freq_dist = Counter(tokens)\n",
    "            \n",
    "            # Get top words\n",
    "            top_words = freq_dist.most_common(max_words)\n",
    "            \n",
    "            # Format as keywords\n",
    "            keywords = []\n",
    "            for word, count in top_words:\n",
    "                # Normalize score between 0 and 1\n",
    "                score = count / max(freq_dist.values()) if freq_dist.values() else 0\n",
    "                keywords.append({\n",
    "                    \"text\": word,\n",
    "                    \"score\": float(score)\n",
    "                })\n",
    "            \n",
    "            return keywords\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting frequent words: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_title(self, text, filename=\"\"):\n",
    "        \"\"\"\n",
    "        Try to extract a title from the document\n",
    "        \n",
    "        Args:\n",
    "            text (str): Document text\n",
    "            filename (str): Original filename\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted title\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return filename\n",
    "            \n",
    "        # First try: Take the first line if it's reasonably short\n",
    "        lines = text.split('\\n')\n",
    "        if lines and lines[0].strip() and len(lines[0].strip()) < 100 and len(lines[0].strip().split()) < 15:\n",
    "            return lines[0].strip()\n",
    "        \n",
    "        # Second try: Look for patterns that might indicate titles\n",
    "        title_patterns = [\n",
    "            r'^#\\s+(.+)$',  # Markdown title\n",
    "            r'^Title:\\s*(.+)$',  # Explicit title\n",
    "            r'^Subject:\\s*(.+)$'  # Document subject\n",
    "        ]\n",
    "        \n",
    "        for pattern in title_patterns:\n",
    "            match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1).strip()\n",
    "        \n",
    "        # Fallback: Use filename without extension or first few words\n",
    "        if filename:\n",
    "            filename_parts = filename.rsplit('.', 1)\n",
    "            if len(filename_parts) > 1:\n",
    "                return filename_parts[0]\n",
    "            \n",
    "        # Last resort: Use first few words\n",
    "        words = text.split()\n",
    "        if len(words) > 5:\n",
    "            return \" \".join(words[:5]) + \"...\"\n",
    "            \n",
    "        return filename or \"Untitled Document\"\n",
    "    \n",
    "    def generate_summary(self, text, max_length=200):\n",
    "        \"\"\"\n",
    "        Generate a brief summary of the document\n",
    "        \n",
    "        Args:\n",
    "            text (str): Document text\n",
    "            max_length (int): Maximum summary length\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated summary\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"Empty document\"\n",
    "            \n",
    "        # Simple extractive summarization\n",
    "        try:\n",
    "            # Split into sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "            \n",
    "            if len(sentences) <= 2:\n",
    "                # If very short document, return the text itself\n",
    "                if len(text) <= max_length:\n",
    "                    return text\n",
    "                return text[:max_length] + \"...\"\n",
    "            \n",
    "            # Use first and last sentence as summary\n",
    "            first_sentence = sentences[0]\n",
    "            last_sentence = sentences[-1]\n",
    "            \n",
    "            # If they're too long, truncate\n",
    "            if len(first_sentence) > max_length // 2:\n",
    "                first_sentence = first_sentence[:max_length // 2] + \"...\"\n",
    "            \n",
    "            if len(last_sentence) > max_length // 2:\n",
    "                last_sentence = last_sentence[:max_length // 2] + \"...\"\n",
    "            \n",
    "            return first_sentence + \" [...] \" + last_sentence\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {e}\")\n",
    "            # Fallback to simple truncation\n",
    "            if len(text) <= max_length:\n",
    "                return text\n",
    "            return text[:max_length] + \"...\"\n",
    "    \n",
    "    def calculate_readability(self, text):\n",
    "        \"\"\"\n",
    "        Calculate simple readability score based on sentence and word length\n",
    "        \n",
    "        Args:\n",
    "            text (str): Document text\n",
    "            \n",
    "        Returns:\n",
    "            float: Readability score (0-100)\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "            \n",
    "        try:\n",
    "            sentences = sent_tokenize(text)\n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if not sentences or not words:\n",
    "                return 0\n",
    "            \n",
    "            # Average sentence length\n",
    "            avg_sentence_length = len(words) / len(sentences)\n",
    "            \n",
    "            # Average word length\n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "            \n",
    "            # Simple readability score (higher is more complex)\n",
    "            # Scale between 0-100 for easier interpretation\n",
    "            readability = (avg_sentence_length * 0.6 + avg_word_length * 5) * 5\n",
    "            \n",
    "            # Cap the score at 100\n",
    "            return min(100, readability)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating readability: {e}\")\n",
    "            return 0\n",
    "\n",
    "# Create an instance of the semantic analyzer\n",
    "semantic_analyzer = SemanticAnalyzer()\n",
    "\n",
    "# Test the semantic analyzer with our sample text\n",
    "print(\"Analyzing sample text...\\n\")\n",
    "\n",
    "# Extract entities\n",
    "entities = semantic_analyzer.extract_entities(cleaned_text)\n",
    "print(\"Extracted entities:\")\n",
    "print(\"-\" * 50)\n",
    "for entity_type, entity_list in entities.items():\n",
    "    if entity_list:\n",
    "        print(f\"{entity_type}: {', '.join(entity_list)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Extract keywords\n",
    "keywords = semantic_analyzer.extract_keywords(cleaned_text)\n",
    "print(\"\\nExtracted keywords:\")\n",
    "print(\"-\" * 50)\n",
    "for keyword in keywords:\n",
    "    print(f\"{keyword['text']} (score: {keyword['score']:.3f})\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Extract title\n",
    "title = semantic_analyzer.extract_title(cleaned_text, \"sample.txt\")\n",
    "print(f\"\\nExtracted title: {title}\")\n",
    "\n",
    "# Generate summary\n",
    "summary = semantic_analyzer.generate_summary(cleaned_text)\n",
    "print(f\"\\nGenerated summary: {summary}\")\n",
    "\n",
    "# Calculate readability\n",
    "readability = semantic_analyzer.calculate_readability(cleaned_text)\n",
    "print(f\"\\nReadability score: {readability:.2f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2785762",
   "metadata": {},
   "source": [
    "## Automated Metadata Generation\n",
    "\n",
    "Now that we can extract text from documents and analyze its semantic content, let's combine these capabilities to automatically generate comprehensive metadata for documents. We'll create a MetadataGenerator class that:\n",
    "\n",
    "1. Takes a document file as input\n",
    "2. Extracts the text content\n",
    "3. Analyzes the content to extract semantic information\n",
    "4. Generates structured metadata\n",
    "\n",
    "The metadata we'll generate includes:\n",
    "- Basic document information (filename, file size, word count)\n",
    "- Content-based information (title, summary, language)\n",
    "- Semantic information (keywords, entities)\n",
    "- Analytical information (readability score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd001d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataGenerator:\n",
    "    \"\"\"Class for generating metadata from documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with document processor and semantic analyzer\"\"\"\n",
    "        self.doc_processor = DocumentProcessor()\n",
    "        self.semantic_analyzer = SemanticAnalyzer()\n",
    "    \n",
    "    def generate_metadata(self, file_path):\n",
    "        \"\"\"\n",
    "        Generate comprehensive metadata for a document\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Generated metadata\n",
    "        \"\"\"\n",
    "        # Extract filename from path\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        # Extract text from document\n",
    "        extracted_text = self.doc_processor.extract_text(file_path)\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        cleaned_text = self.doc_processor.clean_text(extracted_text)\n",
    "        \n",
    "        # If no text was extracted, return basic metadata\n",
    "        if not cleaned_text:\n",
    "            return self._generate_empty_metadata(filename)\n",
    "        \n",
    "        # Calculate word count\n",
    "        word_count = len(word_tokenize(cleaned_text)) if cleaned_text else 0\n",
    "        \n",
    "        # Extract title\n",
    "        title = self.semantic_analyzer.extract_title(cleaned_text, filename)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = self.semantic_analyzer.generate_summary(cleaned_text)\n",
    "        \n",
    "        # Extract keywords\n",
    "        keywords = self.semantic_analyzer.extract_keywords(cleaned_text)\n",
    "        \n",
    "        # Extract entities\n",
    "        entities = self.semantic_analyzer.extract_entities(cleaned_text)\n",
    "        \n",
    "        # Calculate readability\n",
    "        readability_score = self.semantic_analyzer.calculate_readability(cleaned_text)\n",
    "        \n",
    "        # Detect language (simplified - always English for this example)\n",
    "        language = \"English\"\n",
    "        \n",
    "        # Compile metadata\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"title\": title,\n",
    "            \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"file_size\": len(extracted_text),\n",
    "            \"word_count\": word_count,\n",
    "            \"language\": language,\n",
    "            \"summary\": summary,\n",
    "            \"keywords\": keywords,\n",
    "            \"entities\": entities,\n",
    "            \"readability_score\": readability_score\n",
    "        }\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _generate_empty_metadata(self, filename):\n",
    "        \"\"\"Generate metadata for empty or unprocessable document\"\"\"\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"title\": filename,\n",
    "            \"processing_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"file_size\": 0,\n",
    "            \"word_count\": 0,\n",
    "            \"language\": \"unknown\",\n",
    "            \"summary\": \"Empty or unprocessable document\",\n",
    "            \"keywords\": [],\n",
    "            \"entities\": {},\n",
    "            \"readability_score\": 0\n",
    "        }\n",
    "    \n",
    "    def save_metadata(self, metadata, output_format=\"json\", output_dir=\"output\"):\n",
    "        \"\"\"\n",
    "        Save metadata to file in specified format\n",
    "        \n",
    "        Args:\n",
    "            metadata (dict): Metadata to save\n",
    "            output_format (str): Format to save in (json or csv)\n",
    "            output_dir (str): Directory to save to\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to saved file\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create filename based on original filename\n",
    "        base_filename = metadata.get('filename', 'unknown').rsplit('.', 1)[0]\n",
    "        \n",
    "        if output_format.lower() == \"json\":\n",
    "            output_path = os.path.join(output_dir, f\"{base_filename}_metadata.json\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        elif output_format.lower() == \"csv\":\n",
    "            output_path = os.path.join(output_dir, f\"{base_filename}_metadata.csv\")\n",
    "            \n",
    "            # Flatten the metadata for CSV\n",
    "            flat_metadata = {\n",
    "                \"filename\": metadata.get(\"filename\", \"\"),\n",
    "                \"title\": metadata.get(\"title\", \"\"),\n",
    "                \"processing_date\": metadata.get(\"processing_date\", \"\"),\n",
    "                \"file_size\": metadata.get(\"file_size\", 0),\n",
    "                \"word_count\": metadata.get(\"word_count\", 0),\n",
    "                \"language\": metadata.get(\"language\", \"\"),\n",
    "                \"summary\": metadata.get(\"summary\", \"\"),\n",
    "                \"readability_score\": metadata.get(\"readability_score\", 0),\n",
    "                \"keywords\": \", \".join([k.get(\"text\", \"\") for k in metadata.get(\"keywords\", [])]),\n",
    "                \"persons\": \", \".join(metadata.get(\"entities\", {}).get(\"PERSON\", [])),\n",
    "                \"organizations\": \", \".join(metadata.get(\"entities\", {}).get(\"ORG\", [])),\n",
    "                \"locations\": \", \".join(metadata.get(\"entities\", {}).get(\"GPE\", [])),\n",
    "                \"dates\": \", \".join(metadata.get(\"entities\", {}).get(\"DATE\", []))\n",
    "            }\n",
    "            \n",
    "            # Save as CSV\n",
    "            df = pd.DataFrame([flat_metadata])\n",
    "            df.to_csv(output_path, index=False)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output format: {output_format}\")\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "# Create an instance of the metadata generator\n",
    "metadata_gen = MetadataGenerator()\n",
    "\n",
    "# Generate metadata for our sample text file\n",
    "metadata = metadata_gen.generate_metadata('sample_docs/sample.txt')\n",
    "\n",
    "# Print the generated metadata\n",
    "print(\"Generated metadata for sample.txt:\")\n",
    "print(\"-\" * 50)\n",
    "print(json.dumps(metadata, indent=2))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save the metadata to a file\n",
    "json_path = metadata_gen.save_metadata(metadata, \"json\")\n",
    "csv_path = metadata_gen.save_metadata(metadata, \"csv\")\n",
    "\n",
    "print(f\"\\nMetadata saved to {json_path} and {csv_path}\")\n",
    "\n",
    "# Display the CSV content\n",
    "print(\"\\nCSV content:\")\n",
    "print(\"-\" * 50)\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb48a2",
   "metadata": {},
   "source": [
    "## Structured Metadata Output Formatting\n",
    "\n",
    "We've already implemented basic metadata output formatting in the MetadataGenerator class, with support for JSON and CSV formats. Now, let's visualize the metadata and explore how it can be used for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a399f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Visualize metadata using matplotlib\n",
    "    \n",
    "    Args:\n",
    "        metadata (dict): Metadata to visualize\n",
    "    \"\"\"\n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Document title\n",
    "    plt.suptitle(metadata.get('title', 'Unknown Document'), fontsize=16, y=0.98)\n",
    "    \n",
    "    # Basic information\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.axis('off')\n",
    "    info_text = (\n",
    "        f\"Filename: {metadata.get('filename', 'Unknown')}\\n\"\n",
    "        f\"Processing Date: {metadata.get('processing_date', 'Unknown')}\\n\"\n",
    "        f\"File Size: {metadata.get('file_size', 0)} bytes\\n\"\n",
    "        f\"Word Count: {metadata.get('word_count', 0)}\\n\"\n",
    "        f\"Language: {metadata.get('language', 'Unknown')}\\n\"\n",
    "        f\"Readability Score: {metadata.get('readability_score', 0):.2f}/100\"\n",
    "    )\n",
    "    plt.text(0.1, 0.5, info_text, fontsize=10, va='center')\n",
    "    plt.title('Document Information', fontsize=12)\n",
    "    \n",
    "    # Keywords visualization\n",
    "    plt.subplot(3, 2, 2)\n",
    "    keywords = metadata.get('keywords', [])\n",
    "    if keywords:\n",
    "        words = [kw.get('text', '') for kw in keywords]\n",
    "        scores = [kw.get('score', 0) for kw in keywords]\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        words = [words[i] for i in sorted_indices]\n",
    "        scores = [scores[i] for i in sorted_indices]\n",
    "        \n",
    "        # Create horizontal bar chart\n",
    "        y_pos = np.arange(len(words))\n",
    "        plt.barh(y_pos, scores, align='center', color='skyblue')\n",
    "        plt.yticks(y_pos, words)\n",
    "        plt.xlabel('Score')\n",
    "        plt.title('Top Keywords', fontsize=12)\n",
    "    else:\n",
    "        plt.axis('off')\n",
    "        plt.text(0.5, 0.5, 'No keywords found', ha='center', va='center')\n",
    "        plt.title('Top Keywords', fontsize=12)\n",
    "    \n",
    "    # Entities visualization\n",
    "    plt.subplot(3, 2, 3)\n",
    "    entities = metadata.get('entities', {})\n",
    "    \n",
    "    # Count entities by type\n",
    "    entity_counts = {k: len(v) for k, v in entities.items() if v}\n",
    "    \n",
    "    if entity_counts:\n",
    "        # Create pie chart\n",
    "        labels = list(entity_counts.keys())\n",
    "        sizes = list(entity_counts.values())\n",
    "        plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=plt.cm.Paired.colors)\n",
    "        plt.axis('equal')\n",
    "        plt.title('Named Entities by Type', fontsize=12)\n",
    "    else:\n",
    "        plt.axis('off')\n",
    "        plt.text(0.5, 0.5, 'No entities found', ha='center', va='center')\n",
    "        plt.title('Named Entities by Type', fontsize=12)\n",
    "    \n",
    "    # Summary visualization\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.axis('off')\n",
    "    summary = metadata.get('summary', 'No summary available')\n",
    "    plt.text(0.1, 0.5, f\"Summary:\\n\\n{summary}\", fontsize=10, va='center', wrap=True)\n",
    "    plt.title('Document Summary', fontsize=12)\n",
    "    \n",
    "    # Readability gauge\n",
    "    plt.subplot(3, 2, 5)\n",
    "    readability = metadata.get('readability_score', 0)\n",
    "    \n",
    "    # Create a gauge-like visualization\n",
    "    categories = ['Simple', 'Standard', 'Complex']\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    \n",
    "    # Determine which category the score falls into\n",
    "    if readability < 30:\n",
    "        category_index = 0\n",
    "    elif readability < 70:\n",
    "        category_index = 1\n",
    "    else:\n",
    "        category_index = 2\n",
    "    \n",
    "    # Create bar chart for readability\n",
    "    bars = plt.bar([0, 1, 2], [30, 40, 30], color='lightgray')\n",
    "    bars[category_index].set_color(colors[category_index])\n",
    "    \n",
    "    plt.xticks([0, 1, 2], categories)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title(f'Readability: {readability:.1f}/100', fontsize=12)\n",
    "    \n",
    "    # Entity details\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Format entity details as text\n",
    "    entity_text = \"Entity Details:\\n\\n\"\n",
    "    for entity_type, entity_list in entities.items():\n",
    "        if entity_list:\n",
    "            entity_text += f\"{entity_type}:\\n\"\n",
    "            for entity in entity_list[:5]:  # Show only first 5 to avoid clutter\n",
    "                entity_text += f\"- {entity}\\n\"\n",
    "            if len(entity_list) > 5:\n",
    "                entity_text += f\"  ... and {len(entity_list) - 5} more\\n\"\n",
    "            entity_text += \"\\n\"\n",
    "    \n",
    "    plt.text(0.1, 0.5, entity_text, fontsize=10, va='center')\n",
    "    plt.title('Named Entity Details', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to format metadata for different outputs\n",
    "def format_metadata_for_output(metadata, output_format=\"html\"):\n",
    "    \"\"\"\n",
    "    Format metadata for different output formats\n",
    "    \n",
    "    Args:\n",
    "        metadata (dict): Metadata to format\n",
    "        output_format (str): Format to output (html, markdown, xml)\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted metadata\n",
    "    \"\"\"\n",
    "    if output_format.lower() == \"html\":\n",
    "        # Format as HTML\n",
    "        html = f\"\"\"\n",
    "        <div class=\"metadata-container\">\n",
    "            <h2>{metadata.get('title', 'Unknown Document')}</h2>\n",
    "            \n",
    "            <div class=\"metadata-section\">\n",
    "                <h3>Document Information</h3>\n",
    "                <table>\n",
    "                    <tr><th>Filename</th><td>{metadata.get('filename', 'Unknown')}</td></tr>\n",
    "                    <tr><th>Processing Date</th><td>{metadata.get('processing_date', 'Unknown')}</td></tr>\n",
    "                    <tr><th>File Size</th><td>{metadata.get('file_size', 0)} bytes</td></tr>\n",
    "                    <tr><th>Word Count</th><td>{metadata.get('word_count', 0)}</td></tr>\n",
    "                    <tr><th>Language</th><td>{metadata.get('language', 'Unknown')}</td></tr>\n",
    "                    <tr><th>Readability Score</th><td>{metadata.get('readability_score', 0):.2f}/100</td></tr>\n",
    "                </table>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metadata-section\">\n",
    "                <h3>Summary</h3>\n",
    "                <p>{metadata.get('summary', 'No summary available')}</p>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metadata-section\">\n",
    "                <h3>Keywords</h3>\n",
    "                <div class=\"keyword-container\">\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add keywords\n",
    "        for keyword in metadata.get('keywords', []):\n",
    "            html += f\"\"\"<span class=\"keyword\" style=\"opacity: {max(0.5, keyword.get('score', 0))}\">{keyword.get('text', '')}</span>\"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metadata-section\">\n",
    "                <h3>Named Entities</h3>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add entities\n",
    "        for entity_type, entity_list in metadata.get('entities', {}).items():\n",
    "            if entity_list:\n",
    "                html += f\"\"\"\n",
    "                <div class=\"entity-group\">\n",
    "                    <h4>{entity_type}</h4>\n",
    "                    <ul>\n",
    "                \"\"\"\n",
    "                \n",
    "                for entity in entity_list:\n",
    "                    html += f\"\"\"<li>{entity}</li>\"\"\"\n",
    "                \n",
    "                html += \"\"\"\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    elif output_format.lower() == \"markdown\":\n",
    "        # Format as Markdown\n",
    "        md = f\"\"\"# {metadata.get('title', 'Unknown Document')}\n",
    "\n",
    "## Document Information\n",
    "- **Filename:** {metadata.get('filename', 'Unknown')}\n",
    "- **Processing Date:** {metadata.get('processing_date', 'Unknown')}\n",
    "- **File Size:** {metadata.get('file_size', 0)} bytes\n",
    "- **Word Count:** {metadata.get('word_count', 0)}\n",
    "- **Language:** {metadata.get('language', 'Unknown')}\n",
    "- **Readability Score:** {metadata.get('readability_score', 0):.2f}/100\n",
    "\n",
    "## Summary\n",
    "{metadata.get('summary', 'No summary available')}\n",
    "\n",
    "## Keywords\n",
    "\"\"\"\n",
    "        \n",
    "        # Add keywords\n",
    "        for keyword in metadata.get('keywords', []):\n",
    "            md += f\"- {keyword.get('text', '')} ({keyword.get('score', 0):.3f})\\n\"\n",
    "        \n",
    "        md += \"\\n## Named Entities\\n\"\n",
    "        \n",
    "        # Add entities\n",
    "        for entity_type, entity_list in metadata.get('entities', {}).items():\n",
    "            if entity_list:\n",
    "                md += f\"\\n### {entity_type}\\n\"\n",
    "                for entity in entity_list:\n",
    "                    md += f\"- {entity}\\n\"\n",
    "        \n",
    "        return md\n",
    "    \n",
    "    elif output_format.lower() == \"xml\":\n",
    "        # Format as XML\n",
    "        xml = f\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<metadata>\n",
    "    <document>\n",
    "        <title>{metadata.get('title', 'Unknown Document')}</title>\n",
    "        <filename>{metadata.get('filename', 'Unknown')}</filename>\n",
    "        <processing_date>{metadata.get('processing_date', 'Unknown')}</processing_date>\n",
    "        <file_size>{metadata.get('file_size', 0)}</file_size>\n",
    "        <word_count>{metadata.get('word_count', 0)}</word_count>\n",
    "        <language>{metadata.get('language', 'Unknown')}</language>\n",
    "        <readability_score>{metadata.get('readability_score', 0):.2f}</readability_score>\n",
    "    </document>\n",
    "    \n",
    "    <content>\n",
    "        <summary>{metadata.get('summary', 'No summary available')}</summary>\n",
    "        <keywords>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add keywords\n",
    "        for keyword in metadata.get('keywords', []):\n",
    "            xml += f\"\"\"        <keyword score=\"{keyword.get('score', 0):.3f}\">{keyword.get('text', '')}</keyword>\\n\"\"\"\n",
    "        \n",
    "        xml += \"\"\"    </keywords>\n",
    "        <entities>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add entities\n",
    "        for entity_type, entity_list in metadata.get('entities', {}).items():\n",
    "            if entity_list:\n",
    "                xml += f\"\"\"        <entity_group type=\"{entity_type}\">\\n\"\"\"\n",
    "                for entity in entity_list:\n",
    "                    xml += f\"\"\"            <entity>{entity}</entity>\\n\"\"\"\n",
    "                xml += f\"\"\"        </entity_group>\\n\"\"\"\n",
    "        \n",
    "        xml += \"\"\"    </entities>\n",
    "    </content>\n",
    "</metadata>\"\"\"\n",
    "        \n",
    "        return xml\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output format: {output_format}\")\n",
    "\n",
    "# Visualize the metadata\n",
    "visualize_metadata(metadata)\n",
    "\n",
    "# Format the metadata in different formats\n",
    "html_output = format_metadata_for_output(metadata, \"html\")\n",
    "markdown_output = format_metadata_for_output(metadata, \"markdown\")\n",
    "xml_output = format_metadata_for_output(metadata, \"xml\")\n",
    "\n",
    "# Save the formatted outputs\n",
    "with open('output/metadata_output.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_output)\n",
    "\n",
    "with open('output/metadata_output.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(markdown_output)\n",
    "\n",
    "with open('output/metadata_output.xml', 'w', encoding='utf-8') as f:\n",
    "    f.write(xml_output)\n",
    "\n",
    "print(\"Formatted metadata saved to output directory in HTML, Markdown, and XML formats\")\n",
    "\n",
    "# Display the Markdown output as an example\n",
    "print(\"\\nSample Markdown Output:\")\n",
    "print(\"-\" * 50)\n",
    "print(markdown_output[:500] + \"...\" if len(markdown_output) > 500 else markdown_output)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35537caa",
   "metadata": {},
   "source": [
    "## Web Interface for Document Upload and Metadata Display\n",
    "\n",
    "Now that we have a functional metadata generation system, let's design a web interface for users to upload documents and view the generated metadata. We'll use Streamlit for this, which is a fast way to build interactive web applications in Python.\n",
    "\n",
    "Here, we'll provide the code for a Streamlit app. To run this app, you would save it to a separate file and run it with `streamlit run app.py`.\n",
    "\n",
    "**Note:** Since this is a Jupyter notebook, we won't actually run the Streamlit app here, but we'll provide the complete code for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new file called 'streamlit_app.py' with the following content\n",
    "streamlit_app_code = \"\"\"\n",
    "import streamlit as st\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the parent directory to the path to import our modules\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "# Import our metadata generation classes\n",
    "from src.document_processor import DocumentProcessor\n",
    "from src.metadata_generator import MetadataGenerator\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Metamorph - Metadata Generator\",\n",
    "    page_icon=\"ð\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Initialize the metadata generator\n",
    "@st.cache_resource\n",
    "def get_metadata_generator():\n",
    "    return MetadataGenerator()\n",
    "\n",
    "metadata_gen = get_metadata_generator()\n",
    "\n",
    "# App title\n",
    "st.title(\"Metamorph\")\n",
    "st.subheader(\"Automated Metadata Generation System\")\n",
    "st.markdown(\"---\")\n",
    "\n",
    "# Sidebar\n",
    "st.sidebar.header(\"About\")\n",
    "st.sidebar.info(\n",
    "    \"Metamorph automatically extracts and generates metadata from various document types \"\n",
    "    \"including PDF, DOCX, TXT, and images.\"\n",
    ")\n",
    "\n",
    "st.sidebar.header(\"Instructions\")\n",
    "st.sidebar.info(\n",
    "    \"1. Upload a document using the file uploader below.\\n\"\n",
    "    \"2. The system will process the document and extract metadata.\\n\"\n",
    "    \"3. View the generated metadata and visualizations.\\n\"\n",
    "    \"4. Download the metadata in your preferred format.\"\n",
    ")\n",
    "\n",
    "st.sidebar.header(\"Supported File Types\")\n",
    "st.sidebar.markdown(\n",
    "    \"- PDF (.pdf)\\n\"\n",
    "    \"- Word (.docx)\\n\"\n",
    "    \"- Text (.txt)\\n\"\n",
    "    \"- Images (.png, .jpg, .jpeg)\"\n",
    ")\n",
    "\n",
    "# File upload\n",
    "st.header(\"Upload Document\")\n",
    "uploaded_file = st.file_uploader(\"Choose a file\", type=[\"pdf\", \"docx\", \"txt\", \"png\", \"jpg\", \"jpeg\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Display progress\n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "    \n",
    "    # Update progress\n",
    "    status_text.text(\"Uploading file...\")\n",
    "    progress_bar.progress(20)\n",
    "    \n",
    "    # Save the uploaded file to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{uploaded_file.name.split('.')[-1]}\") as tmp_file:\n",
    "        tmp_file.write(uploaded_file.getvalue())\n",
    "        temp_file_path = tmp_file.name\n",
    "    \n",
    "    # Update progress\n",
    "    status_text.text(\"Extracting text...\")\n",
    "    progress_bar.progress(40)\n",
    "    \n",
    "    # Generate metadata\n",
    "    status_text.text(\"Analyzing content...\")\n",
    "    progress_bar.progress(60)\n",
    "    \n",
    "    metadata = metadata_gen.generate_metadata(temp_file_path)\n",
    "    \n",
    "    # Update progress\n",
    "    status_text.text(\"Generating metadata...\")\n",
    "    progress_bar.progress(80)\n",
    "    \n",
    "    # Save metadata to JSON temporarily\n",
    "    json_path = os.path.join(tempfile.gettempdir(), \"metadata.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Save metadata to CSV temporarily\n",
    "    csv_path = os.path.join(tempfile.gettempdir(), \"metadata.csv\")\n",
    "    metadata_gen.save_metadata(metadata, \"csv\", tempfile.gettempdir())\n",
    "    \n",
    "    # Update progress\n",
    "    status_text.text(\"Done!\")\n",
    "    progress_bar.progress(100)\n",
    "    \n",
    "    # Display metadata\n",
    "    st.header(\"Generated Metadata\")\n",
    "    \n",
    "    # Basic info in columns\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    with col1:\n",
    "        st.subheader(\"Document Information\")\n",
    "        st.markdown(f\"**Title:** {metadata.get('title', 'Unknown')}\")\n",
    "        st.markdown(f\"**Filename:** {metadata.get('filename', 'Unknown')}\")\n",
    "        st.markdown(f\"**Word Count:** {metadata.get('word_count', 0)}\")\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"Processing Information\")\n",
    "        st.markdown(f\"**Processing Date:** {metadata.get('processing_date', 'Unknown')}\")\n",
    "        st.markdown(f\"**Language:** {metadata.get('language', 'Unknown')}\")\n",
    "        st.markdown(f\"**File Size:** {metadata.get('file_size', 0)} bytes\")\n",
    "    \n",
    "    with col3:\n",
    "        st.subheader(\"Analysis\")\n",
    "        readability = metadata.get('readability_score', 0)\n",
    "        st.markdown(f\"**Readability Score:** {readability:.2f}/100\")\n",
    "        \n",
    "        # Readability gauge\n",
    "        if readability < 30:\n",
    "            category = \"Simple\"\n",
    "            color = \"green\"\n",
    "        elif readability < 70:\n",
    "            category = \"Standard\"\n",
    "            color = \"orange\"\n",
    "        else:\n",
    "            category = \"Complex\"\n",
    "            color = \"red\"\n",
    "            \n",
    "        st.markdown(f\"**Complexity:** <span style='color:{color}'>{category}</span>\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Summary\n",
    "    st.subheader(\"Document Summary\")\n",
    "    st.write(metadata.get('summary', 'No summary available'))\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Keywords and entities\n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Keywords\")\n",
    "        keywords = metadata.get('keywords', [])\n",
    "        if keywords:\n",
    "            # Display as a table\n",
    "            keyword_df = pd.DataFrame(keywords)\n",
    "            st.dataframe(keyword_df)\n",
    "            \n",
    "            # Create a bar chart\n",
    "            fig, ax = plt.subplots(figsize=(10, 4))\n",
    "            keyword_df = keyword_df.sort_values('score', ascending=False)\n",
    "            ax.barh(keyword_df['text'], keyword_df['score'], color='skyblue')\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_title('Top Keywords')\n",
    "            st.pyplot(fig)\n",
    "        else:\n",
    "            st.write(\"No keywords found\")\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"Named Entities\")\n",
    "        entities = metadata.get('entities', {})\n",
    "        \n",
    "        # Display entities by type\n",
    "        for entity_type, entity_list in entities.items():\n",
    "            if entity_list:\n",
    "                with st.expander(f\"{entity_type} ({len(entity_list)})\"):\n",
    "                    st.write(\", \".join(entity_list))\n",
    "        \n",
    "        # Create a pie chart of entity distribution\n",
    "        entity_counts = {k: len(v) for k, v in entities.items() if v}\n",
    "        if entity_counts:\n",
    "            fig, ax = plt.subplots(figsize=(8, 8))\n",
    "            ax.pie(entity_counts.values(), labels=entity_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
    "            ax.axis('equal')\n",
    "            st.pyplot(fig)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Download options\n",
    "    st.header(\"Download Metadata\")\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    \n",
    "    with col1:\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_data = f.read()\n",
    "        st.download_button(\n",
    "            label=\"Download JSON\",\n",
    "            data=json_data,\n",
    "            file_name=f\"{metadata.get('filename', 'metadata')}.json\",\n",
    "            mime=\"application/json\"\n",
    "        )\n",
    "    \n",
    "    with col2:\n",
    "        with open(csv_path, 'r') as f:\n",
    "            csv_data = f.read()\n",
    "        st.download_button(\n",
    "            label=\"Download CSV\",\n",
    "            data=csv_data,\n",
    "            file_name=f\"{metadata.get('filename', 'metadata')}.csv\",\n",
    "            mime=\"text/csv\"\n",
    "        )\n",
    "    \n",
    "    with col3:\n",
    "        # Generate markdown\n",
    "        markdown_data = f\"# {metadata.get('title', 'Unknown Document')}\\n\\n\"\n",
    "        markdown_data += f\"**Filename:** {metadata.get('filename', 'Unknown')}  \\n\"\n",
    "        markdown_data += f\"**Processing Date:** {metadata.get('processing_date', 'Unknown')}  \\n\"\n",
    "        markdown_data += f\"**Word Count:** {metadata.get('word_count', 0)}  \\n\"\n",
    "        markdown_data += f\"**Readability Score:** {metadata.get('readability_score', 0):.2f}/100  \\n\\n\"\n",
    "        \n",
    "        markdown_data += f\"## Summary\\n{metadata.get('summary', 'No summary available')}\\n\\n\"\n",
    "        \n",
    "        markdown_data += \"## Keywords\\n\"\n",
    "        for keyword in metadata.get('keywords', []):\n",
    "            markdown_data += f\"- {keyword.get('text', '')} ({keyword.get('score', 0):.3f})\\n\"\n",
    "        \n",
    "        markdown_data += \"\\n## Named Entities\\n\"\n",
    "        for entity_type, entity_list in metadata.get('entities', {}).items():\n",
    "            if entity_list:\n",
    "                markdown_data += f\"\\n### {entity_type}\\n\"\n",
    "                for entity in entity_list:\n",
    "                    markdown_data += f\"- {entity}\\n\"\n",
    "        \n",
    "        st.download_button(\n",
    "            label=\"Download Markdown\",\n",
    "            data=markdown_data,\n",
    "            file_name=f\"{metadata.get('filename', 'metadata')}.md\",\n",
    "            mime=\"text/markdown\"\n",
    "        )\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    try:\n",
    "        os.unlink(temp_file_path)\n",
    "        os.unlink(json_path)\n",
    "        os.unlink(csv_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "else:\n",
    "    # Show sample when no file is uploaded\n",
    "    st.info(\"Upload a document to generate metadata\")\n",
    "    \n",
    "    # Sample image\n",
    "    st.image(\"https://via.placeholder.com/800x400.png?text=Metamorph+Automated+Metadata+Generation\", use_column_width=True)\n",
    "    \n",
    "    # Sample features\n",
    "    st.header(\"Key Features\")\n",
    "    \n",
    "    feature_col1, feature_col2 = st.columns(2)\n",
    "    \n",
    "    with feature_col1:\n",
    "        st.markdown(\"#### Content Extraction\")\n",
    "        st.markdown(\"- Extract text from PDF, DOCX, TXT\")\n",
    "        st.markdown(\"- OCR for images and scanned documents\")\n",
    "        st.markdown(\"- Support for multiple languages\")\n",
    "        \n",
    "        st.markdown(\"#### Semantic Analysis\")\n",
    "        st.markdown(\"- Named entity recognition\")\n",
    "        st.markdown(\"- Keyword extraction\")\n",
    "        st.markdown(\"- Document summarization\")\n",
    "    \n",
    "    with feature_col2:\n",
    "        st.markdown(\"#### Metadata Generation\")\n",
    "        st.markdown(\"- Comprehensive document metadata\")\n",
    "        st.markdown(\"- Readability assessment\")\n",
    "        st.markdown(\"- Content classification\")\n",
    "        \n",
    "        st.markdown(\"#### Output Options\")\n",
    "        st.markdown(\"- JSON, CSV, Markdown formats\")\n",
    "        st.markdown(\"- Data visualizations\")\n",
    "        st.markdown(\"- Integration with other systems\")\n",
    "\"\"\"\n",
    "\n",
    "# Save the Streamlit app to a file\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(\"Streamlit app code has been saved to 'streamlit_app.py'\")\n",
    "print(\"To run the app, use the command: streamlit run streamlit_app.py\")\n",
    "\n",
    "# Display a code snippet for educational purposes\n",
    "print(\"\\nPreview of the Streamlit app code:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\n\".join(streamlit_app_code.split(\"\\n\")[:30]) + \"\\n...\")  # Show first 30 lines\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78084a3",
   "metadata": {},
   "source": [
    "## System Deployment\n",
    "\n",
    "Deploying the Metamorph system involves two components:\n",
    "1. The Flask web application (from the main project)\n",
    "2. The Streamlit application (as an alternative interface)\n",
    "\n",
    "Below, we'll provide instructions for deploying both components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment instructions file\n",
    "deployment_instructions = \"\"\"# Deployment Instructions for Metamorph\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- pip (Python package installer)\n",
    "- Git\n",
    "- Tesseract OCR (for image processing)\n",
    "\n",
    "## Step 1: Clone the Repository\n",
    "```bash\n",
    "git clone https://github.com/yourusername/Metamorph.git\n",
    "cd Metamorph\n",
    "```\n",
    "\n",
    "## Step 2: Set up a Virtual Environment\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate the virtual environment\n",
    "# On Windows:\n",
    "venv\\\\Scripts\\\\activate\n",
    "# On macOS/Linux:\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "## Step 3: Install Dependencies\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Step 4: Install Tesseract OCR\n",
    "- For Ubuntu/Debian:\n",
    "  ```\n",
    "  sudo apt-get install tesseract-ocr\n",
    "  ```\n",
    "- For macOS:\n",
    "  ```\n",
    "  brew install tesseract\n",
    "  ```\n",
    "- For Windows: Download and install from [GitHub](https://github.com/UB-Mannheim/tesseract/wiki)\n",
    "\n",
    "## Step 5: Download spaCy Model\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Option 1: Deploy the Flask Web Application\n",
    "\n",
    "### Local Deployment\n",
    "```bash\n",
    "# Start the Flask app\n",
    "python app.py\n",
    "```\n",
    "\n",
    "The application will be available at http://localhost:5000\n",
    "\n",
    "### Production Deployment with Gunicorn (Linux/macOS)\n",
    "```bash\n",
    "pip install gunicorn\n",
    "gunicorn -w 4 -b 0.0.0.0:5000 app:app\n",
    "```\n",
    "\n",
    "### Docker Deployment\n",
    "1. Create a Dockerfile:\n",
    "```Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    tesseract-ocr \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "RUN python -m spacy download en_core_web_sm\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"app:app\"]\n",
    "```\n",
    "\n",
    "2. Build and run the Docker container:\n",
    "```bash\n",
    "docker build -t metamorph .\n",
    "docker run -p 5000:5000 metamorph\n",
    "```\n",
    "\n",
    "## Option 2: Deploy the Streamlit Web Application\n",
    "\n",
    "### Local Deployment\n",
    "```bash\n",
    "pip install streamlit\n",
    "streamlit run streamlit_app.py\n",
    "```\n",
    "\n",
    "The application will be available at http://localhost:8501\n",
    "\n",
    "### Production Deployment\n",
    "For production deployment of Streamlit apps, consider using:\n",
    "- [Streamlit Sharing](https://streamlit.io/sharing)\n",
    "- [Heroku](https://heroku.com)\n",
    "- [AWS](https://aws.amazon.com)\n",
    "- [Google Cloud Platform](https://cloud.google.com)\n",
    "\n",
    "Example for Heroku:\n",
    "1. Create a `Procfile`:\n",
    "```\n",
    "web: streamlit run streamlit_app.py --server.port $PORT\n",
    "```\n",
    "\n",
    "2. Deploy to Heroku:\n",
    "```bash\n",
    "heroku create metamorph-app\n",
    "git push heroku main\n",
    "```\n",
    "\n",
    "## Option 3: Run as a Jupyter Notebook\n",
    "\n",
    "The `notebooks/metadata_generation.ipynb` can be run in a Jupyter environment:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Navigate to the notebooks directory and open `metadata_generation.ipynb`.\n",
    "\n",
    "## Security Considerations for Production\n",
    "\n",
    "1. **Secure File Uploads**: Implement strict file validation and virus scanning\n",
    "2. **Rate Limiting**: Prevent abuse with rate limiting\n",
    "3. **User Authentication**: Add authentication for production deployments\n",
    "4. **HTTPS**: Always use HTTPS in production\n",
    "5. **Environment Variables**: Store sensitive information in environment variables\n",
    "6. **Regular Updates**: Keep all dependencies updated\n",
    "\n",
    "## Maintenance\n",
    "\n",
    "1. Regularly update dependencies\n",
    "2. Monitor application logs\n",
    "3. Set up automated backups for any databases\n",
    "4. Implement monitoring for server health\n",
    "5. Create a process for user feedback and bug reporting\n",
    "\"\"\"\n",
    "\n",
    "# Save the deployment instructions\n",
    "with open('deployment_instructions.md', 'w') as f:\n",
    "    f.write(deployment_instructions)\n",
    "\n",
    "print(\"Deployment instructions have been saved to 'deployment_instructions.md'\")\n",
    "\n",
    "# Create a sample Dockerfile\n",
    "dockerfile_content = \"\"\"FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    tesseract-ocr \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "RUN python -m spacy download en_core_web_sm\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"app:app\"]\n",
    "\"\"\"\n",
    "\n",
    "# Save the Dockerfile\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Sample Dockerfile has been created\")\n",
    "\n",
    "# Create a Procfile for Heroku\n",
    "with open('Procfile', 'w') as f:\n",
    "    f.write(\"web: gunicorn app:app\")\n",
    "\n",
    "print(\"Procfile for Heroku deployment has been created\")\n",
    "\n",
    "# Show deployment options\n",
    "print(\"\\nDeployment Options:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"1. Local Flask App: python app.py\")\n",
    "print(\"2. Local Streamlit App: streamlit run streamlit_app.py\")\n",
    "print(\"3. Docker: docker build -t metamorph . && docker run -p 5000:5000 metamorph\")\n",
    "print(\"4. Heroku: git push heroku main\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862f85d",
   "metadata": {},
   "source": [
    "## Generate README.md File\n",
    "\n",
    "Finally, let's generate a comprehensive README.md file for the project. This will serve as documentation for users and developers who want to understand and use the Metamorph system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive README.md file\n",
    "readme_content = \"\"\"# Metamorph: Automated Metadata Generation System\n",
    "\n",
    "![Metamorph](https://via.placeholder.com/800x200.png?text=Metamorph+Metadata+Generation)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Metamorph is a comprehensive automated metadata generation system designed to enhance document discoverability, classification, and analysis. The system processes various document formats, extracts meaningful content, and generates structured metadata that can be used for better document management and insights.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Multi-format Support**: Process documents in PDF, DOCX, TXT, and image formats\n",
    "- **Automatic Content Extraction**: Extract text using specialized parsers for each format\n",
    "- **OCR Capabilities**: Convert text in images to machine-readable content\n",
    "- **Semantic Analysis**: Identify meaningful sections and key information in documents\n",
    "- **Named Entity Recognition**: Extract people, organizations, locations, and dates\n",
    "- **Keyword Extraction**: Generate relevant keywords using TF-IDF analysis\n",
    "- **Readability Assessment**: Calculate document complexity scores\n",
    "- **Document Summarization**: Generate concise summaries of document content\n",
    "- **Intuitive Web Interface**: Upload documents and view generated metadata\n",
    "- **Analytics Dashboard**: View insights across processed documents\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "Metamorph is built with a modern tech stack:\n",
    "\n",
    "- **Backend**: Python/Flask RESTful API\n",
    "- **Frontend**: HTML5, CSS3, JavaScript with Bootstrap 5\n",
    "- **Text Processing**: NLTK, spaCy, scikit-learn\n",
    "- **Document Parsing**: PyPDF2, python-docx, Pillow, pytesseract\n",
    "- **Data Analysis**: pandas, NumPy\n",
    "- **Visualization**: Chart.js\n",
    "\n",
    "## Installation and Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Tesseract OCR (for image processing)\n",
    "- Required Python packages (see requirements.txt)\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```\n",
    "   git clone https://github.com/yourusername/Metamorph.git\n",
    "   cd Metamorph\n",
    "   ```\n",
    "\n",
    "2. Create and activate a virtual environment (recommended):\n",
    "   ```\n",
    "   python -m venv venv\n",
    "   source venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n",
    "   ```\n",
    "\n",
    "3. Install dependencies:\n",
    "   ```\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "4. Install additional requirements for OCR:\n",
    "   - For Ubuntu/Debian:\n",
    "     ```\n",
    "     sudo apt-get install tesseract-ocr\n",
    "     ```\n",
    "   - For macOS:\n",
    "     ```\n",
    "     brew install tesseract\n",
    "     ```\n",
    "   - For Windows: Download and install from [GitHub](https://github.com/UB-Mannheim/tesseract/wiki)\n",
    "\n",
    "5. Download spaCy model:\n",
    "   ```\n",
    "   python -m spacy download en_core_web_sm\n",
    "   ```\n",
    "\n",
    "6. Run the application:\n",
    "   ```\n",
    "   python app.py\n",
    "   ```\n",
    "\n",
    "7. Access the web interface at http://localhost:5000\n",
    "\n",
    "## Using the Jupyter Notebook\n",
    "\n",
    "For data scientists and developers who want to use the core functionality in a notebook environment, we provide a comprehensive Jupyter Notebook that demonstrates:\n",
    "\n",
    "1. Document loading and text extraction\n",
    "2. Text preprocessing and cleaning\n",
    "3. Feature extraction and metadata generation\n",
    "4. Visualization of results\n",
    "5. Examples of advanced usage\n",
    "\n",
    "The notebook can be found at `notebooks/metadata_generation.ipynb`.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "Metamorph/\n",
    "âââ app.py                  # Main Flask application\n",
    "âââ requirements.txt        # Python dependencies\n",
    "âââ README.md               # Project documentation\n",
    "âââ Dockerfile              # Docker configuration\n",
    "âââ Procfile                # Heroku configuration\n",
    "âââ streamlit_app.py        # Streamlit interface\n",
    "âââ deployment_instructions.md  # Deployment guide\n",
    "âââ notebooks/              # Jupyter notebooks\n",
    "â   âââ metadata_generation.ipynb\n",
    "âââ src/                    # Source code\n",
    "â   âââ document_processor.py  # Document text extraction\n",
    "â   âââ metadata_generator.py  # Metadata generation logic\n",
    "âââ static/                 # Static files (CSS, JS, images)\n",
    "â   âââ css/\n",
    "â   âââ js/\n",
    "â   âââ img/\n",
    "âââ templates/              # HTML templates\n",
    "â   âââ index.html\n",
    "â   âââ analyze.html\n",
    "âââ uploads/                # Temporary document storage\n",
    "```\n",
    "\n",
    "## API Reference\n",
    "\n",
    "Metamorph provides a simple RESTful API for programmatic access:\n",
    "\n",
    "- `POST /upload`: Upload and process a document\n",
    "  - Returns: JSON with generated metadata\n",
    "\n",
    "- `GET /analyze`: Get analytics data across all processed documents\n",
    "  - Returns: JSON with aggregate statistics\n",
    "\n",
    "## Alternative Interfaces\n",
    "\n",
    "### Streamlit Interface\n",
    "\n",
    "We also provide a Streamlit-based interface that can be run separately:\n",
    "\n",
    "```\n",
    "streamlit run streamlit_app.py\n",
    "```\n",
    "\n",
    "This interface provides the same core functionality with a different user experience.\n",
    "\n",
    "## Contributing\n",
    "\n",
    "Contributions are welcome! Please feel free to submit a Pull Request.\n",
    "\n",
    "1. Fork the repository\n",
    "2. Create your feature branch (`git checkout -b feature/amazing-feature`)\n",
    "3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n",
    "4. Push to the branch (`git push origin feature/amazing-feature`)\n",
    "5. Open a Pull Request\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the LICENSE file for details.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- NLTK and spaCy for natural language processing\n",
    "- PyPDF2 and python-docx for document parsing\n",
    "- Flask for the web framework\n",
    "- Bootstrap for the UI components\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save the README.md to the project root\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md has been generated and saved to the project root\")\n",
    "\n",
    "# Display conclusion\n",
    "print(\"\\nThis completes our Metamorph - Automated Metadata Generation System notebook.\")\n",
    "print(\"You can now explore the system further and customize it for your specific needs.\")\n",
    "print(\"To run the web application, use: python app.py\")\n",
    "print(\"To run the Streamlit interface, use: streamlit run streamlit_app.py\")\n",
    "print(\"\\nThank you for using Metamorph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fa04d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've built a comprehensive automated metadata generation system from scratch. We've:\n",
    "\n",
    "1. Implemented text extraction from various document formats (PDF, DOCX, TXT)\n",
    "2. Added OCR capabilities for images and scanned documents\n",
    "3. Created semantic analysis for identifying meaningful content\n",
    "4. Developed a metadata generation pipeline\n",
    "5. Designed output formats for structured metadata\n",
    "6. Created a web interface for document upload and metadata viewing\n",
    "7. Provided deployment instructions for various environments\n",
    "\n",
    "The Metamorph system can be used in a variety of contexts:\n",
    "- Document management systems\n",
    "- Digital libraries and archives\n",
    "- Content management systems\n",
    "- Data governance and compliance\n",
    "- Research repositories\n",
    "- Enterprise search systems\n",
    "\n",
    "You can extend this system by:\n",
    "- Adding support for more document formats\n",
    "- Implementing more advanced NLP techniques\n",
    "- Enhancing the web interface with additional features\n",
    "- Adding a database for storing processed documents and metadata\n",
    "- Implementing user authentication and access control\n",
    "\n",
    "Thank you for exploring the Metamorph Automated Metadata Generation System!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
